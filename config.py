tqdm_colors = ["#0867C5", "green", "blue", "red", "yellow", "#6707B0", "#FFDD22", "#00FFEF", "#DC442C"]
data_dir = "./data"
image_res = 64
image_dir = f"{data_dir}/Images"
resized_img_dir = f"{data_dir}/ResizedImages_{image_res}"
vae_batch_size = 1
vae_group_size = 4
vae_num_epochs = 1000
vae_stopping_patience = 50
vae_hidden_dim_1 = 32 # 32, 64
vae_latent_channels = 256 # 256, 512
vae_beta_kld = 1e-1
vae_optim_lr = 1e-4
vae_lambda_tvl = 1e-3
vae_dropout = 0.
vae_checkpoint_dir = "./checkpoints/vae"
vae_weight = f"test_best_{image_res}_3_32_128_256_beta_{vae_beta_kld}_tvl_{vae_lambda_tvl}"
latent_dir = f"./latents_{image_res}_test"
num_layers = 2
latent_dim = image_res // (2 ** num_layers)
# ////////////////////////////////////////////////
text_captions = f"{data_dir}/captions.txt"
unet_batch_size = 1
embedding_dim = 1024
embedding_dir = f"./embeddings_{embedding_dim}"
embedding_model = "ViT-g-14" # or "ViT-B-16"
embedding_pretrained = "laion2b_s12b_b42k" # or "openai"
unet_checkpoint_dir = "./checkpoints/ddpm"
unet_num_epochs = 1000
unet_stopping_patience = 50
unet_optim_lr = 1e-4
unet_group_size = 4
unet_dropout = 0.
unet_train_timesteps = 1000